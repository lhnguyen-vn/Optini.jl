var documenterSearchIndex = {"docs":
[{"location":"tutorials/#Tutorials","page":"Tutorials","title":"Tutorials","text":"","category":"section"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"Optini is designed to be easy to use, with a minimal API that allows you to try out  different algorithms. Here you can find a few examples on how to get started with the  package.","category":"page"},{"location":"tutorials/#Rosenbrock-Function","page":"Tutorials","title":"Rosenbrock Function","text":"","category":"section"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"In this tutorial, we use the Rosenbrock function from mathbbR^2 to mathbbR as the  objective to minimize. Rosenbrock has only one global minimum, but it is located in a narrow valley that resembles the shape of a banana. Finding this trench is quite trivial, but it  can be rather difficult to converge to the minimum as the bottom of the valley is flat. We  first define the function as follows:","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"julia> rosenbrock(x; a=1, b=100) = (a - x[1])^2 + b*(x[2] - x[1]^2)^2\nrosenbrock (generic function with 1 method)","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"We then select an appropriate algorithm. At the moment, Optini provides support for generic line search and trust region methods, with convenient aliases for popular algorithms. In  this instance, we will use the famous gradient descent method:","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"julia> using Optini\n\njulia> alg = GradientDescent()\nLine Search Algorithm:\n  • Direction: Steepest()\n  • Initial step length guess: StaticInitial{Float64}(0.001)\n  • Step length optimization method: BacktrackingLineSearch{Float64,Float64}(0.0001, 0.5)","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"Optini informs us that the default GradientDescent is a LineSearch method using the  Steepest direction, a static initial step length 0.001, and BacktrackingLineSearch to  search for the appropriate step. All of these parameters are modular, and you can refer to  the documentation on Line Search Algorithms to select the appropriate methods or  customize your own.","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"Once we have defined the algorithm, the other necessary ingredient is an initial point to  kickstart our process. Here, we choose the origin:","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"julia> x = zeros(2)\n2-element Array{Float64,1}:\n 0.0\n 0.0","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"Finally, we simply call the optimize function:","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"julia> optimize(rosenbrock, x; alg)\nSolution Summary:\n  • Converged: true\n  • Total Iterations: 65897\n  • Minimizer: [0.9999999999988823, 0.9999999999977602]\n  • Minimum: 1.2511394230708787e-24","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"Considering the analytical minimum is 0 attained at [1, 1], the solution is by and large accurate. The optimize interface also includes a few solver options, such as  absolute tolerance for the gradient norm or the maximum number of iterations. For instance,  if we want to save the trace of the solver:","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"julia> sol = optimize(rosenbrock, x; alg, savetrace=true);\n\njulia> sol.metadata[:trace][1:20_000:end]\n4-element Array{Optini.FirstOrderState{Array{Float64,1},Float64},1}:\n Optini.FirstOrderState{Array{Float64,1},Float64}([0.0, 0.0], 1.0, [-2.0, 0.0])\n Optini.FirstOrderState{Array{Float64,1},Float64}([0.9998977748147321, 0.9997951509901917], 1.0466723905409629e-8, [-4.083139381724218e-5, -8.181785220440219e-5])\n Optini.FirstOrderState{Array{Float64,1},Float64}([0.9999999653270628, 0.9999999305153797], 1.2041376497141562e-15, [-1.384702634028865e-8, -2.774942498007249e-8])\n Optini.FirstOrderState{Array{Float64,1},Float64}([0.9999999999882364, 0.9999999999764259], 1.3860259910853339e-22, [-4.742206627604874e-12, -9.392486788328824e-12])","category":"page"},{"location":"man/univariate/#Univariate-Optimization","page":"Univariate","title":"Univariate Optimization","text":"","category":"section"},{"location":"man/univariate/","page":"Univariate","title":"Univariate","text":"Optini provides a collection of popular bracketing and root-finding algorithms including  Bisection, Fibonacci, GoldenSection, and QuadraticFit. The optimize call in this case requires the lower and upper bounds of the search  interval, for which Optini provides a convenient bracket function to find an  initial bracket around the minimum. ","category":"page"},{"location":"man/univariate/#References","page":"Univariate","title":"References","text":"","category":"section"},{"location":"man/univariate/","page":"Univariate","title":"Univariate","text":"optimize(::Function, ::Real, ::Real)\nbracket\nBisection\nFibonacci\nGoldenSection\nQuadraticFit","category":"page"},{"location":"man/univariate/#Optini.optimize-Tuple{Function,Real,Real}","page":"Univariate","title":"Optini.optimize","text":"optimize(f::Function, lower::Real, upper::Real; kwargs...)\n\nCompute the minimizer of the function f using the bracket (lower, upper).\n\nKeywords\n\nalg::UnivariateAlgorithm=GoldenSection(): the algorithm choice\nreltol=sqrt(eps(T)): relative tolerance\nabstol=eps(T): absolute tolerance\nmaxiter::Integer=1_000: maximum numer of iterations\n\n\n\n\n\n","category":"method"},{"location":"man/univariate/#Optini.bracket","page":"Univariate","title":"Optini.bracket","text":"bracket(f::Function, x::T=0.0; kwargs...) where {T<:AbstractFloat}\n\nCreate a bracket around a local minimum from the intial point x. Return the bracket in a  tuple if successful, and nothing otherwise. \n\nKeywords\n\nstep=0.01: the step size to expand the bracket\nscale=2.0: the scale factor for step size at each iteration\nmaxiter::Integer=100: the maximum number of iterations\n\n\n\n\n\n","category":"function"},{"location":"man/univariate/#Optini.Bisection","page":"Univariate","title":"Optini.Bisection","text":"Bisection <: UnivariateAlgorithm\n\nBisection search is a root-finding algorithm by maintainining a bracket of opposing  derivative signs. At each iteration the algorithm shrinks the bracket by half until a critical point is located or the bracket is sufficiently small. \n\n\n\n\n\n","category":"type"},{"location":"man/univariate/#Optini.Fibonacci","page":"Univariate","title":"Optini.Fibonacci","text":"Fibonacci{T} <: UnivariateAlgorithm\n\nThe Fibonacci search algorithm optimize a univariate function by iteratively shrinking the  initial bracket around the minimum. The ratio is computed from the Fibonacci sequence.\n\n\n\n\n\n","category":"type"},{"location":"man/univariate/#Optini.GoldenSection","page":"Univariate","title":"Optini.GoldenSection","text":"GoldenSection <: UnivariateAlgorithm\n\nThe golden section search algorithm optimize a univariate function by iteratively shrinking  the initial bracket around the minimum with the golden ratio as an approximation to the  Fibonacci sequence.\n\n\n\n\n\n","category":"type"},{"location":"man/univariate/#Optini.QuadraticFit","page":"Univariate","title":"Optini.QuadraticFit","text":"QuadraticFit{T} <: UnivariateAlgorithm\n\nThe quadratic fit algorithm constructs a quadratic function from three points at each iteration as an approximation to the objective. The analytical solution to the quadratic  model is examined as the new minimizer candidate to update the three points. \n\n\n\n\n\n","category":"type"},{"location":"man/multivariate/trustregion/#Trust-Region-Algorithms","page":"Trust Region Algorithms","title":"Trust Region Algorithms","text":"","category":"section"},{"location":"man/multivariate/trustregion/","page":"Trust Region Algorithms","title":"Trust Region Algorithms","text":"TrustRegion","category":"page"},{"location":"man/multivariate/trustregion/#Optini.TrustRegion","page":"Trust Region Algorithms","title":"Optini.TrustRegion","text":"TrustRegion{H, M, D, R<:Ref, E} <: MultivariateAlgorithm\n\nTrustRegion builds a quadratic model within a radius from the current point to approximate the objective function and uses the model to choose the next step.\n\nFields\n\nhessian::H: the method to compute or approximate the Hessian\nmethod::M: the method to compute the appropriate next step\nΔ₀::D: the initial radius\nΔₘₐₓ::D: the maximum radius\nη::E: the threshold to control when TrustRegion will take a step\nηₛ::E: the threshold to control when TrustRegion shrinks the current radius\nηₑ::E: the threshold to control when TrustRegion expands the current radius\nσₛ::S: the factor used to shrink the trust region radius\nσₑ::S: the factor used to expand the trust region radius\nΔ::R: a Ref containing the current radius\n\n\n\n\n\n","category":"type"},{"location":"man/multivariate/linesearch/#Line-Search-Algorithms","page":"Line Search Algorithms","title":"Line Search Algorithms","text":"","category":"section"},{"location":"man/multivariate/linesearch/","page":"Line Search Algorithms","title":"Line Search Algorithms","text":"For the mathematical background behind line search algorithms, this Pluto notebook  details a quick overview with proof of convergence. Optini's [LineSearch] abstraction provides a simple interface to customize and experiment with the basic components: the search direction,  the initial step length guess, and the method to select an appropriate step length. In particular, a few examples from Nocedal & Wright's Numerical Optimization have already been implemented:","category":"page"},{"location":"man/multivariate/linesearch/","page":"Line Search Algorithms","title":"Line Search Algorithms","text":"Search direction: Steepest, Newton\nInitial guess: StaticInitial, PreviousDecreaseInitial, QuadraticInitial\nStep length selection method: StaticLineSearch, ExactLineSearch, BacktrackingLineSearch, InterpolationLineSearch, StrongWolfeLineSearch","category":"page"},{"location":"man/multivariate/linesearch/#Search-Direction-Interface","page":"Line Search Algorithms","title":"Search Direction Interface","text":"","category":"section"},{"location":"man/multivariate/linesearch/","page":"Line Search Algorithms","title":"Line Search Algorithms","text":"Function Input Output\norder YourType order of information required, e.g. Optini.SecondOrder() for Newton direction\ndirection YourType, state the search direction","category":"page"},{"location":"man/multivariate/linesearch/","page":"Line Search Algorithms","title":"Line Search Algorithms","text":"To define a custom search direction, you must implement order and direction on your type. The former tells Optini whether your method requires first-order and/or second-order information, while the latter computes the direction for line search. Optini will compute each iteration's state with the objective value, first-order and/or second-order information necessary for  your algorithm. The following example, for instance, is how Optini defines the steepest  descent direction internally:","category":"page"},{"location":"man/multivariate/linesearch/","page":"Line Search Algorithms","title":"Line Search Algorithms","text":"import Optini: order, direction, FirstOrder\n\n# Define singleton type for dispatch purposes\nstruct Steepest end\n\n# Steepest descent requires first-order information\norder(::Steepest) = FirstOrder()\n\n# Since `Steepest` is now classified as `FirstOrder`, Optini will prepare `FirstOrderState`\n# information at each iteration. We access the gradient in the field `∇f`, and simply define\n# the search direction as the opposite of the gradient.\ndirection(::Steepest, state) = -state.∇f","category":"page"},{"location":"man/multivariate/linesearch/#Initial-Step-Length-Interface","page":"Line Search Algorithms","title":"Initial Step Length Interface","text":"","category":"section"},{"location":"man/multivariate/linesearch/","page":"Line Search Algorithms","title":"Line Search Algorithms","text":"Function Input Output\nreset! YourType reset the method, especially for initial guess dependent on previous iterations\nupdate! YourType, state, p, α update the method, especially for initial guess dependent on previous iterations\nYourType state, p the initial step length guess","category":"page"},{"location":"man/multivariate/linesearch/","page":"Line Search Algorithms","title":"Line Search Algorithms","text":"A custom initial step length can be implemented by defining reset!, update!, and a functor to compute the initial guess. For example, a fixed step length guess might be implemented as:","category":"page"},{"location":"man/multivariate/linesearch/","page":"Line Search Algorithms","title":"Line Search Algorithms","text":"# For simplicity we're only defining `Float64` initial here. Internally, however, Optini \n# uses the parametric struct `StaticInitial{T}`.\nstruct StaticInitial\n    α::Float64\nend\n\n# Static initials don't hold any information that needs to be reset. If you are subtyping \n# `Optini.AbstractInitial{T}`, this is already the fallback behavior.\nfunction reset!(::StaticInitial) end\n\n# Some initial methods require information about the last search direction `p` and step \n# length `α`, to choose an initial for the current iteration and therefore need to be updated. \n# `StaticInitial`, however, doesn't need to update anything here. Similar to `reset!`, this \n# is the default fallback behavior for types subtyping `Optini.AbstractInitial{T}`.\nfunction update!(::StaticInitial, state, p, α) end\n\n# Return the initial step length guess, given the current `state` and the direction `p`\n(si::StaticInitial)(state, p) = si.α","category":"page"},{"location":"man/multivariate/linesearch/#Step-Length-Selection-Method-Interface","page":"Line Search Algorithms","title":"Step Length Selection Method Interface","text":"","category":"section"},{"location":"man/multivariate/linesearch/","page":"Line Search Algorithms","title":"Line Search Algorithms","text":"Function Input Output\nYourType f, state, p, α₀ the appropriate step length","category":"page"},{"location":"man/multivariate/linesearch/","page":"Line Search Algorithms","title":"Line Search Algorithms","text":"A custom step length selection scheme receives the objective function f, the current state,  the search direction p, and the initial step length α₀ to produce the next step length.  For instance, the following code defines a fixed line search method:","category":"page"},{"location":"man/multivariate/linesearch/","page":"Line Search Algorithms","title":"Line Search Algorithms","text":"struct StaticLineSearch end\n\n# Static line search method simply uses the initial step length\n(sls::StaticLineSearch)(f, state, p, α₀) = α₀","category":"page"},{"location":"man/multivariate/linesearch/#References","page":"Line Search Algorithms","title":"References","text":"","category":"section"},{"location":"man/multivariate/linesearch/","page":"Line Search Algorithms","title":"Line Search Algorithms","text":"LineSearch\nGradientDescent\nNewtonDescent\nStaticInitial\nPreviousDecreaseInitial\nQuadraticInitial\nStaticLineSearch\nExactLineSearch\nBacktrackingLineSearch\nInterpolationLineSearch\nStrongWolfeLineSearch","category":"page"},{"location":"man/multivariate/linesearch/#Optini.LineSearch","page":"Line Search Algorithms","title":"Optini.LineSearch","text":"LineSearch{D, I, M} <: MultivariateAlgorithm\n\nLineSearch optimizes the objective function by first choosing a direction, then search  along this direction for an appropriate step. \n\nFields\n\ndirection::D: determines the search direction\ninitial::I: the method to guess an initial step length\nmethod::: the method to compute the appropriate next step\n\n\n\n\n\n","category":"type"},{"location":"man/multivariate/linesearch/#Optini.GradientDescent","page":"Line Search Algorithms","title":"Optini.GradientDescent","text":"GradientDescent{I, M}\n\nAliasing LineSearch with steepest descent.\n\n\n\n\n\n","category":"type"},{"location":"man/multivariate/linesearch/#Optini.NewtonDescent","page":"Line Search Algorithms","title":"Optini.NewtonDescent","text":"NewtonDescent{I, M}\n\nAliasing LineSearch with Newton method.\n\n\n\n\n\n","category":"type"},{"location":"man/multivariate/linesearch/#Optini.StaticInitial","page":"Line Search Algorithms","title":"Optini.StaticInitial","text":"StaticInitial{T} <: AbstractInitial{T}\n\nStatic initial α₀ for line search methods. Newton and quasi-Newton algorithms for instance should use α₀ = 1 as the initial guess.\n\n\n\n\n\n","category":"type"},{"location":"man/multivariate/linesearch/#Optini.PreviousDecreaseInitial","page":"Line Search Algorithms","title":"Optini.PreviousDecreaseInitial","text":"PreviousDecreaseInitial{T, D<:Ref} <: AbstractInitial{T}\n\nPreviousDecreaseInitial assumes the first-order decrease is the same as the previous iteration:\n\nα₀ = αₖ₁fracf^intercal_k-1 p_k-1f^intercal_k p_k\n\nFields\n\nα::T: default step length for the first iteration\nprev_decrease::D: save the last iteration's first-order decrease\n\n\n\n\n\n","category":"type"},{"location":"man/multivariate/linesearch/#Optini.QuadraticInitial","page":"Line Search Algorithms","title":"Optini.QuadraticInitial","text":"QuadraticInitial{T} <: AbstractInitial{T}\n\nQuadraticInitial assumes ϕ(α₀) - ϕ(0) = fₖ - fₖ₁ and uses quadratic interpolation with ϕ(α₀), ϕ(0), ϕ(0) to compute the initial guess.\n\nFields\n\nα::T: default step length for the first iteration\nprev_f::Ref{T}: save the last iteration's function value\n\n\n\n\n\n","category":"type"},{"location":"man/multivariate/linesearch/#Optini.StaticLineSearch","page":"Line Search Algorithms","title":"Optini.StaticLineSearch","text":"StaticLineSearch\n\nStaticLineSearch directly uses the initial guess as the new step length.\n\n\n\n\n\n","category":"type"},{"location":"man/multivariate/linesearch/#Optini.ExactLineSearch","page":"Line Search Algorithms","title":"Optini.ExactLineSearch","text":"ExactLineSearch{A<:UnivariateAlgorithm, O1, O2}\n\nExactLineSearch computes the optimal step length using univariate methods. Since most  univariate algorithms require repeated calculations of the objective to shrink the bracket around a local minimum, ExactLineSearch is expected to be expensive.\n\nFields\n\nalg::A: the univariate algorithm\nbracket_options::O1: named tuple of bracketing options\nalg_options::O2: named tuple of the univariate algorithm options\n\n\n\n\n\n","category":"type"},{"location":"man/multivariate/linesearch/#Optini.BacktrackingLineSearch","page":"Line Search Algorithms","title":"Optini.BacktrackingLineSearch","text":"BacktrackingLineSearch{C, T}\n\nBacktrackingLineSearch starts from an initial step length guess and gradually backtracks  until the Armijo sufficient decrease condition is met.\n\nFields\n\nc::C: the constant for the Armijo condition in the open interval (0, 1)\nscale::T: the scale factor to shrink the step length\n\n\n\n\n\n","category":"type"},{"location":"man/multivariate/linesearch/#Optini.InterpolationLineSearch","page":"Line Search Algorithms","title":"Optini.InterpolationLineSearch","text":"InterpolationLineSearch{C, T}\n\nInterpolationLineSearch uses quadratic and cubic interpolations to compute the step length satisfying the Armijo sufficient decrease condition.\n\nFields\n\nc::C: the constant for the Armijo condition in the open interval (0, 1)\nϵ::T: the minimum step length decrease from the initial step length\n\n\n\n\n\n","category":"type"},{"location":"man/multivariate/linesearch/#Optini.StrongWolfeLineSearch","page":"Line Search Algorithms","title":"Optini.StrongWolfeLineSearch","text":"StrongWolfeLineSearch{C1, C2, S, T}\n\nStrongWolfeLineSearch computes a step length that satifies both the Armijo sufficient  decrease and the curvature conditions.\n\nFields\n\nc₁::C1: Armijo condition constant\nc₂::C2: curvature condition constant\nscale::S: scale factor to expand step length bracket\nα_max::T: maximum step length\n\n\n\n\n\n","category":"type"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = Optini","category":"page"},{"location":"#Optini","page":"Home","title":"Optini","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Optini is a prototype numerical optimization package for pedagogical and research purposes.  It primarily references Nocedal & Wright's Numerical Optimization and Kochenderfer & Wheeler's Algorithms for Optimization. The main purpose of Optini is to provide an extensible but generic interface for non-linear  optimization algorithms and simplify the prototype process for new methods. As such, it is  a useful scratchpad for experimental algorithms, and should not be considered production-ready. For state-of-the-art Julia libraries in this space, we recommend Optim.jl and BlackBoxOptim.jl instead. ","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Optini is only compatible with Julia 1.5 and above. If you would like to experiment or  contribute to the package, you can install it from your Julia REPL:","category":"page"},{"location":"","page":"Home","title":"Home","text":"]add https://github.com/lhnguyen-vn/Optini.jl","category":"page"}]
}
